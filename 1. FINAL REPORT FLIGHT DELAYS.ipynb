{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30de0787-3e61-4200-a2b4-1384276c09a7",
     "showTitle": false,
     "title": ""
    },
    "id": "pYj0LOr0KtIA"
   },
   "source": [
    "# Flight Delay Prediction: Analysis and Implementation\n",
    "\n",
    "Link to **this** notebook in Databricks: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836269142/command/3984215836269143\n",
    "\n",
    "Link to **EDA** notebook in Databricks:\n",
    "https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632306754\n",
    "\n",
    "Link to **data pipeline** notebook in Databricks: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632306347\n",
    "\n",
    "Link to **Hyperparameter Tuning Random Forest and Gradient Boosted Tree** Notebooks in Databricks: \n",
    "Random Forest and LightGBM: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836255312\n",
    "\n",
    "Link to **Hyperparamter Tuning Neural Network**: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836270281/command/3984215836270291\n",
    "\n",
    "Link to **Final Runs: Experiments 1-3 (BEFORE DOWNSAMPLING)** Notebook in Databricks:\n",
    "https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836277610/command/3984215836278009\n",
    "\n",
    "Link to **Final Runs Experiments 4-5 (WITH DOWNSAMPLING)** Notebook in Databricks:\n",
    "https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632306707\n",
    "\n",
    "Link to **Gap Analysis** notebook in Databricks:\n",
    "https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632305542"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d358317-33fe-46a2-932b-b5f983704823",
     "showTitle": true,
     "title": "Phase Leader & Team"
    },
    "id": "AETnpYFSmHFV"
   },
   "source": [
    "- Team 3-2 \n",
    "  - Phase 4 Leader - Jess\n",
    "  - W261 - Summer 2023, Section 5\n",
    "\n",
    "|| | | |\n",
    "|-------|-------|-------|-------|\n",
    "| Kisha Kim <br> kisha.kim@berkeley.edu <br> **Lead in Phase 1** <br> Plan the Project <br> <br> ![Kisha](https://i.imgur.com/LeBnAhBb.jpg) | Chase Madson <br> chase_madson@berkeley.edu <br> **Lead in Phase 2** <br> Pipelines and Baselines <br> <br> ![Chase](https://i.imgur.com/1YjZSlwb.jpg) | Eric Danforth <br> edanforth85@berkeley.edu <br> **Lead in Phase 3** <br> Improve our Baseline <br> <br> ![Eric](https://i.imgur.com/f9pKc1Sb.jpg) | Jess Stockham <br> jhsmith@berkeley.edu <br> **Lead in Phase 4** <br> Find Optimal Algorithm <br> <br> ![Jess](https://i.imgur.com/TNc2GXKb.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd63677-a3a5-46fc-8f81-33acf030db55",
     "showTitle": true,
     "title": "Credit Assignment Plan Link"
    },
    "id": "1D5ch76qmTBP"
   },
   "source": [
    "***https://docs.google.com/spreadsheets/d/1s9vRM4yVHMS86UlCq4orkgHVhLRaXO_E3X_TzdnWbew/edit#gid=0***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27070038-f0d0-4775-aa82-de65ca567a84",
     "showTitle": true,
     "title": "Time Allocated to Each Task"
    }
   },
   "source": [
    "![Pipeline](https://i.imgur.com/1wwsXPY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e4c919-5133-4fc2-ab26-5184b5089296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# I. Abstract\n",
    "\n",
    "This project develops a machine learning model that predicts flight delays based on historical flight, airport station, and weather data spanning five years from 2015-2019 in the United States.  A flight delay is a flight that departs at least 15 minutes past its scheduled time, per the FAA definition. We evaluate the usefulness of our model with a metric that is appropriate for the nature of the data and the business context. Flight delays are relatively rare (about 20 percent of flights) which suggests a focus on either precision or recall. Specifically, we use the f-beta evaluation metric that strikes a balance between recall and precision, but places greater weight on recall to score our various predictive models.  We prioritize recall, which is the number of flights we correctly predicted as delayed divided by all the flights that were truly delayed. This biases us to train a model that errors on the side of predicting more flight delays and being wrong because we do not want to miss any potential delays. It is easier for airlines and customers to recover logistically and reputationally from warning customers there may be a delay and being wrong. This is in contrast to using a more conservative metric that prioritizes precision that only announces a flight delay if we are extremely sure it will happen. A f-beta score ranges from 0 to 100% and an ideal f-beta score is 100%, but a good f-beta score would be between 80% and 100%.\n",
    "\n",
    "We use data from 2015-2018 to build a model so we can predict flight delays on an unseen dataset of all flights in 2019. This is a big-data problem as the dataset has approximately 31 million rows. We process the data on a distributed cloud computing environment. We built models with 81 features (numeric and categorical) across multiple estimators using data from 2015-2018: logistic regression, random forest, a variant of gradient boosted trees, and neural networks. We also applied innovative approaches such as downsampling the majority class to address the class inbalance and engineering sophisticated graph-based features such as pagerank and triangle count. Our data pipeline is carefully designed to avoid data leakage that would harm the predictive power of the model. Data leakage is a common hazard of machine learning, particularly with time-series data, where researchers use information that would not be available during training to the test set.\n",
    "\n",
    "In machine learning, you build a model based on data from your training set, in our case from 2015-2018, and then test your model on an unseen test set, in our case 2019 data. Our baseline model was a simple random forest classifier a small set of predictors and default parameters using 3 months of training data from 2015, which had a f-beta score of 0%, which is equivalent to the naive baseline that assumes all flights are on-time. Our final model shows marked improvement from baseline, using a more predictive set of features and downsampling the majority class, predicting the full 2015-2018 training with a f-beta of 63%. We used this final model built on the training set to predict the unseen, held-out 2019 data, which is the best and final assessment of the power of our model, achieving a middling f-beta score of 54%. In our current form, our model's f-beta metric is insufficient to provide a useful tool for industry.  This implies that our features are not rich enough to represent the complex set of factors that cause flight delays. The report's conclusion discusses several directions for future work to improve this predictive tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce5b0723-27c9-4872-9057-a159ff07f735",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# II. Business Case\n",
    "\n",
    "On-time Performance (OTP) for air travel has been in a state of steady decline. The Bureau of Transportations statistics show that for the most recent 15+ years of reported data, while weather delays have reduced their overall percentage impact (newer fleets and better technology), air carrier delays have more than made up the difference and the same problems that were evident in the early 2000s worsened during the pandemic and have not improved post-COVID. ([Source](https://www.bts.gov/topics/airlines-and-airports/understanding-reporting-causes-flight-delays-and-cancellations))\n",
    "\n",
    "The cost to society is not only felt in a direct economic impact to the cost of travel, but in spillover effects to tourism and local economies. Major studies have indicated that this has been caused by the rapid expansion of air carriers. ([Source](https://www-jstor-org.libproxy.berkeley.edu/stable/ee727ffb-b4c6-34ac-9f85-5070b8fe6c05?seq=14)) However, the last decade or so has been marked by mergers - Delta and Northwest (2010), United and Continental (2010), Southwest and AirTran (2011), American Airlines and US Airways (2013), and the recently stalled Frontier and Spirit merger which may result in Spirit and JetBlue joining forces. ([Source](https://www.sheffield.com/2022/the-recent-history-of-u-s-airline-mergers.html), [Source](https://www.forbes.com/sites/marisadellatto/2022/07/27/spirit-and-frontier-airlines-cancel-merger-plans-opening-door-for-jetblues-offer/?sh=4e4cc0723fa1)) Despite all of the streamlining in operations, OTP continues to lag behind.\n",
    "\n",
    "While the airline industry expects to be profitable for the first time since the start of the pandemic in 2023 ([Source](https://www.cnn.com/2023/06/05/business/iata-airlines-profits-post-pandemic/index.html)), time is money and airlines need to improve this key performance metric if they want to recover from multi-year losses. The most recent estimates by the FAA in 2019 show the cost of airline delays steadily rising from $19.2B in 2012 to an estimated $33B in 2019. ([Source](https://www.faa.gov/air_traffic/by_the_numbers/media/Air_Traffic_by_the_Numbers_2023.pdf)) \n",
    "\n",
    "This project develops a predictive tool to predict flight delays. To align with the Federal Aviation Administration's definition, our flight delay outcome metric is a departure time that occurs more than 15 minutes than originally scheduled. We will make predictions 2 hours prior to the flight's scheduled departure time. Importantly, this means we cannot include any data/features in our predictive model that we would ONLY know at the last minute, or more specifically, ONLY know within two hours of the flight's scheduled departure. This is a **classification problem**, where the positive class (y=1) is a flight departure delay and the negative class (y=0) is an on-time flight departure.\n",
    "\n",
    "We have **imbalanced classes**, where only roughly 20% of flights are delayed (y=1) and 80% are on-time (y=1), and so we tailored our evaluation metric to the nature of the data. Specifically, we evaluated the performance of our model using a **F-beta score, where beta=2**. F-scores balance both precision and recall. Our variant on the F-score, places greater weight on recall and less weight on precision because of the business context.  We focus on recall, which is the number of flights we correctly predicted as delayed divided by all the flights that were truly delayed. This biases us to train a model that errors on the side of predicting more flight delays and being wrong because we do not want to miss any potient delays. This is in contrast to using a more conservative metric focused on precision that only announces a flight delay if we are extremely sure it will happen. We focus on recall because is easier for airlines and customers to recover logistically and reputationally from warning customers there may be a delay and being wrong, than the reverse. F-scores range from 0% to 100%, with 100% as the ideal metric. The most basic baseline is to assume that all flights are on-time (not delayed) and that assumes a recall and f-beta score of 0%.\n",
    "\n",
    "The f-beta score with beta=2 formula is given by:\n",
    "\n",
    "$$ F_\\beta = \\frac{(1 + \\beta^2) \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a534d40-a05e-4fe1-b231-4bafe0c8ce2b",
     "showTitle": false,
     "title": ""
    },
    "id": "xhA0UyrOLIPy"
   },
   "source": [
    "# III. Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d8c806-10e1-488c-a534-75f71649e068",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this section, we describe our target variable, the dataset, our exploratory data analysis, feature engineering, and final set of features for the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8c494c-dfd8-49e8-b784-26d800b5dbef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A. Data\n",
    "\n",
    "**Target Variable**. The target variable is `DEP_DEL15`, which takes on the value 1 if the flight departed 15 minutes or later than the scheduled departure time, and a 0 otherwise. As discussed above, most flights depart on time, leading to an imbalanced dataset where only about 1 out of 5 flights are delayed.\n",
    "\n",
    "**Data Size**. The dataset contains all domestic United States flights from the years 2015-2019 and contains approximately 31 million rows. This is a large enough dataset that we consider it \"big data,\" and processed the data in an distributed cloud computing environment (Azure) with 36 cores with 4 worker machines (32 cores) and 1 driver (4 cores). As a basic cleaning step, we removed \"CANCELED\" flights (1.5% of flights), which never took off, and out of scope for our analysis.\n",
    "\n",
    "**Data Features**. From this dataset, we have 215 potential flight attributes available to us with which to predict a delayed departure. We describe the variable groups below.\n",
    "* 63 flight-related features from the [Reporting Carrier On-Time Performance Dataset (BTS)](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ), which contains the following groupings:\n",
    "  - **Time Period**: The date of the flight is stored in `FL_DATE`, accompanied by 5 derived time attribute features: year, quarter, month, day-of-month, and day-of-week. We selected `DAY_OF_WEEK`, `MONTH`, and `YEAR` as features.\n",
    "  - **Airline**: There are 5 features about the airline, the aircraft, and the flight number. We select the airline carrier (e.g., Southwest vs Delta) `OP_UNIQUE_CARRIER` as a feature.\n",
    "  - **Origin & Destination**: There are 20 features describing the flight's points of origin and destination (e.g., airport, city, state). We selected each flight's `ORIGIN` and `DEST` as features. This is a large categorical variable with hundreds of possible values.\n",
    "  - **Departure & Arrival Performance**: Along with the target variable `DEP_DEL15` itself, there are 17 features describing how the flight departed and landed. Most of these will not be available at prediction-time and therefore are not useful for us. We only used `CRS_DEP_TIME`, which is the scheduled departure time from the Central Reservation System and known at prediction-time, and binned this variable into time groups (e.g., early morning). This is described in the feature engineering section below.\n",
    "  - **Flight Summaries**: There are 6 features used to summarize how the flight went (e.g., total distance, time in the air, number of flights). We used `DISTANCE` as a feature or the distance between airports in miles, as this information will be known at prediction-time. \n",
    "  - **Cause of Delay, Cancelations & Diversions**, **Gate Return Information at Origin Airport**: Information about the cause of delay, cancelations and diversions, and gate return information would not be available to us at prediction time and so are outside our scope. \n",
    "* 123 weather-related features brought over from the [Local Climatological Dataset (NOAA)](https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf). We ultimately selected only `ELEVATION` and `AIRPORT_TYPE` (e.g., large, medium, small, seaplane-based) that describe the weather station closest to the airport, as general weather indicators. The hourly and daily weather data across dozens of indicators was too complex for this short analysis. We may develop more advanced weather features in the future.\n",
    "\n",
    "**Exploratory Data Analysis** Exploratory data analysis helps the researcher understand the trends in each potential feature. We examined the distribution of the data in our features (e.g., histograms, summary statistics) and how categorical features varied with the outcome variable, reviewed levels of missingness, and checked for duplicates. We present several charts to depict how features vary with the outcome variable below on the 2015-2018 dataset. In summary, these trends highlight the importance of time-based features to represent seasonality changes (e.g., time of day, day of week, month of year) and the airline carrier for our modeling. We visualize a few of our findings below:\n",
    "\n",
    "![Time Series Decomposition](https://i.imgur.com/Fk3DlKd.png)\n",
    "\n",
    "**Key Insights**\n",
    "* The trend in delay rates dropped below 17% around 2016 but reverted to a more typical 18% afterwards, showcasing a persistent pattern. \n",
    "* Throughout any given year, the delay rate fluctuates about 4% in either direction. Notably, Summer and holiday months often exhibit the highest delays, while Spring and Fall tend to experience below-average delays.\n",
    "* Even after decomposing the time series to remove trend and seasonality, we still see about a 2% swing in the delay rate in any given month. \n",
    "\n",
    "![Delay Rates by Time of Week](https://i.imgur.com/hlQgpGS.png)\n",
    "\n",
    "**Key Insights**\n",
    "* Afternoon flights tend to have the most delay.\n",
    "* Early morning flights see much more variation, likely due to their smaller relative frequency.\n",
    "* Evening flights on Thursday and Fridays have almost a one-third chance of being delayed, suggesting potential operational challenges during these time frames.\n",
    "\n",
    "![Delay Rates by Airline](https://i.imgur.com/k8hBK5R.png)\n",
    "\n",
    "**Key Insights**\n",
    "* Most smaller airlines fall above the overall delay rate, with JetBlue (B6) and Frontier (F9) being especially prone to delays.\n",
    "* Larger airlines like Delta (DL) and American Airlines (AA) are more consistent and fall closer to the overall delay rate.\n",
    "* Hawaii Airlines (HA) and PSA (OH) seem to be better at minimizing delays.\n",
    "\n",
    "**Missing Data**. We assessed the level of missingness on our features we used in this analysis (or variables we used to engineer new features) in the 2015-2018 dataset. The only variable in this set with missing values was AIR_TIME, which we used to engineer a new feature, which had roughly one-quarter of its values missing. We decided to impute the mean to fill in missing values for numeric features. We used mean rather than median, as median is less computationally burdensome for the big data world. \n",
    "\n",
    "**Duplicates**. We checked for duplicates across a composite key (carrier, flight number, origin, flight date, tail number, and scheduled departure time) and found zero duplicates in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c0509f-ac79-4dce-85f2-0091f931f5a3",
     "showTitle": false,
     "title": ""
    },
    "id": "FBcxory8LNij"
   },
   "source": [
    "### B. Feature Engineering\n",
    "\n",
    "We created several additional features including (1) an indicator for if the flight date is a holiday, (2) an indicator for whether the flight is the first of the day, and (3) binning the scheduled departure dates into chunks (early morning, morning, afternoon, early evening, late at night) to indicate time of day, as well as the graph features and time-based features described below. \n",
    "\n",
    "**Graph Features.** Domestic flights between airports can be effectively represented as a network, where airports serve as nodes and the flights between them act as directed links. This alternative way to represent the data allows us to harness the power of graph algorithms to gain valuable insights into the characteristics of airports and their connections.\n",
    "\n",
    "As an innovative approach, one of the key algorithms we employ is **PageRank**, which helps us assess the importance of an airport within the network based on the number of inbound flights it receives. By understanding the PageRank of an airport, we can identify major hubs and critical transit points in the network. Moreover, an airport's **Triangle Count** assists in revealing the structural properties of the network, highlighting airports with a high number of interconnected flights. Additionally, we attempted to use **Label Propagation Analysis (LPA)** to uncover potential communities or clusters of airports with similar traffic patterns and shed light on regional connectivity and passenger flow dynamics, however this feature proved computationally taxing and the number of labels difficult to reproduce on validation and test data. \n",
    "\n",
    "To implement this, we convert our flight data into a graph format and use the algorithms available in the GraphFrames library. Importantly, we mitigate data leakage by computing these metrics solely on the training data, and subsequently, we integrate the values into the test set based on origin and destination airports. Specifically, we implement this at the training sets of each fold during cross-validation. This generated six new features - two sets for each of PageRank, LPA, and Triangle Count. This approach allows us to enhance the understanding of airport characteristics and improve the accuracy of our predictive models for various airport-related analyses. We ultimately dropped LPA from our feature set because it was too compute-intensive.\n",
    "\n",
    "**Flight Delay Duration (from prior flight that day)**. Flight delays from earlier in the day can have a ripple effect on subsequent flights. This field showcases the average delay in minutes from the same day for the same tail number. We discuss this feature further in the data leakage section of the report.\n",
    "\n",
    "**Average Flight Delay Duration (from prior day)**. Some flights undergo similar delays (e.g. if there was a repair in one route, it may impact the delay the next day). This field showcases the average delay in minutes from the previous day for the same flight number.\n",
    "\n",
    "**Average Flight Duration by Flight Number (from prior day)**. Flight duration can help predict flight delay, as long flights might be more prone to delays due to the complexity in coordinating longer routes and schedules. As we are predicting flight delay prior to knowing the current flight duration, we use the average flight duration from the previous day for the same route. \n",
    "\n",
    "**Expected Airport Congestion**. The number of flights scheduled at the airport at the departure time block could impact flight delay. Higher number of flights scheduled from the same airport at the same time could lead to flight delays due to flight coordination/traffic complexities. Since scheduled flight information is retrievable prior to 2 hours, there were no lags used to create this new feature.\n",
    "\n",
    "### 3. Data Dictionary for Features Selected\n",
    "Below, we show a data dictionary for the features included in our modeling. Notably, ORIGIN and DEST were only part of the baseline model, but dropped when we did further experiments. These two features are enormous categorical variables that represent every airport in the United States. Instead, as we moved forward from the baseline, we engineered several other features that can capture airport characteristics in a more efficient manner, including pagerank, triangle count, and a measure of airport congestion that day. In the table below, we use an \\* to indicate features we added after the baseline model. We included all features, with the exception of ORIGIN and DEST, in the hypertuning and final model runs.\n",
    "\n",
    "**Table 1. Feature Data Dictionary**\n",
    "|Feature|\tType|\tExample Data|Feature Type | Description| \n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "|MONTH|\tInt\t|1| Categorical Feature| Month of flight (Jan = 1, ...) | \n",
    "|DAY_OF_WEEK|\tInt\t|4| Categorical Feature| Day of flight (Mon = 1, ...) |\n",
    "|YEAR|\tInt\t|4| Categorical Feature| Year of flight (2015 = 1, ...) |\n",
    "|OP_UNIQUE_CARRIER|\tstring\t|AA| Categorical Feature| Unique AITA Code for Airline | \n",
    "|ORIGIN (basline model only)|\tString\t|SFO| Categorical Feature| AITA Code for the Airport that Flight Departs From | \n",
    "|DEST (basline model only)|\tString\t|JFK| Categorical Feature| AITA Code for the Airport that Flight Arrives In |\n",
    "|DISTANCE|\tInt\t|2427| Numerical Feature| Miles Between Origin and Destination Airports |\n",
    "|origin_type|\tstring\t| medium_airport| Categorical Features| Whether Origin Airport is Big/Med/Small |\n",
    "|dest_type|\tstring\t|medium_airport| Categorical Features| Whether Destination Airport is Big/Med/Small |\n",
    "|ELEVATION|\tInt\t|1462| Numerical Features| Elevation of Station Weather Reported |\n",
    "|CRS_DEP_BUCKET|\tBinned into ranges\t|0900-1200| Categorical Features| The Scheduled Departure Time, Bucketized |\n",
    "|is_holiday\\* | Int | 1|  Categorical Feature | If the flight date is a holiday\n",
    "|is_holiday_adjacent\\* | Int | 1|  Categorical Feature | If the flight date is the day before or the day after a holiday\n",
    "|pagerank_origin\\*, pagerank_dest\\* | Double| 0.9876 | Numerical Feature | Importance of airport based on inbound flights\n",
    "|triangle_count_origin\\*, triangle_count_dest\\* | Double | 3.0 | Numerical Feature | Higlights airports with a large number of interconnected flights\n",
    "|FE_PRIOR_DAILY_AVG_DEP_DELAY\\* | Double |8.5  | Numerical Feature| Average Flight Delay Duration (from prior day) |\n",
    "|DEP_DELAY_LAG\\* | Double | 15.0 | Numerical Feature| For the aircraft slated to make the current trip we seek to predict, this provides the departure delay (in minutes) of its preceding flight |\n",
    "|FE_PRIOR_AVG_DURATION\\* | Double |56.70454 | Numerical Feature| Average Flight Duration by Flight Number (from prior day) |\n",
    "|FE_NUM_FLIGHT_SCHEDULED\\* | Integer | 55 | Numerical Feature | Expected Airport Congestion |\n",
    "\n",
    "\\* Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd10eee6-756d-4885-bab7-693370894200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# IV. Data Leakage\n",
    "Data Leakage is when we use information from outside the training set to build our model. In time-series data, we must be particularly careful to not use data from the future (that we would not have at the time of prediction 2 hours before the flight's scheduled departure) to build our model. Data leakage causes less accurate prediction tools. Our data pipeline, as described below in Section IV is carefully designed to minimize leakage. For example, we apply the meta-data from our training set to the validation set (or the test set, depending on the pipeline stage) to impute missing and normalize numeric data, as well as to create our graph features. In addition, our cross-validation is a time-series blocked design where the folds do not overlap, which restricts any data leakage from the future.\n",
    "\n",
    "Equally important, when we did feature selection, we were careful to select and create new features with information that we would know more than two hours before the flight. We did keep one feature in the model that may have a minimal amount of leakage: DEP_DELAY_LAG. This attribute captures the departure delay (in minutes) of an aircraft's immediately preceding flight — the one directly preceding the flight we seek to predict. Admittedly, a fraction of short-haul flights may take off within two hours of their subsequent scheduled departure, rendering this information unavailable at prediction-time in some cases. However, we believe this introduces an inconsequential amount of data leakage to our model. In actual practice, we would follow a more sophisticated flowchart to determine whether the aircraft was on-schedule or not to eliminate leakage. However, the advanced version of this feature entails several resource-intensive operations — including verifying the flight's arrival status, confirming the flight's departure from the prior airport, converting from local time to UTC, and more. Given these complexities, we are content with the minimal leakage it introduces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "707e5c3c-5e21-4230-b7c8-dbd2b881464c",
     "showTitle": false,
     "title": ""
    },
    "id": "ST0KHEtx-3Uk"
   },
   "source": [
    "# V: Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ed77eda-f236-458e-9afa-296aaed567ab",
     "showTitle": false,
     "title": ""
    },
    "id": "ECWxHX5sRQCz"
   },
   "source": [
    "### A. Data Processing Steps\n",
    "We intentionally designed a data pipeline to avoid data leakage. We describe our data pipeline in detail in the image below. In STEP 1, we cleaned the full 2015-2019 dataset. We dropped canceled flights, created new features, and one-hot encoded categorical features. In STEP 2, we then split the dataset into a train (2015-2018) and test dataset (2019). The test dataset is held out until the very last step of the pipeline (step 7).\n",
    "\n",
    "In STEP 3, we then prepared the data for cross-validation hyper-parameter tuning by splitting the training set into folds, using time-series block evaluation. The folds are split by time interval and the time intervals do not overlap. The folds cover the full 2015-2018 timespan. For example, we created 5 folds in the 2015-2018 dataset. Each fold contained a \"CV training dataset\" of 200 days and \"CV validation dataset\" has the next 92 days after that. The idea of cross-validation is that we have multiple sets of train and validation sets so we do not overfit our model to one particular set of training data. Models that overfit the training set do not generalize well to unseen data. We calculate the cross-validation f-beta as the average f-beta score across all folds for each set of hyperparameters. Notably, for the baseline model measurement, we split the 3-month, Jan - March 2015 dataset into 3 folds where each training set was 20 days and the validation set was the subsequent 9 days.\n",
    "\n",
    "We take a careful approach when performing data cleaning on each of the CV folds. Specifically, we do not want the broader information from the full dataset to leak into that particular CV folds. If that would happen, that risks information about the future being available incorrectly in the present during training. This data leakage would produce worse predictive models. To address this, we performed three data cleaning steps at the fold level, what we call Stage 2 Data Cleaning: (1) missing imputation, (2) standardization of numeric features and (3) creating our graph features. Specifically, we applied the meta-data from each CV training dataset to the corresponding validation dataset set within each fold.\n",
    "\n",
    "In STEP 4, we estimated several models to get the Cross Validation metrics with all our features: random forest, XGBoost (LightGBM package), and neural networks. In the future, we plan to also try ensemble models. We adopted a bayesian hypertuning strategy appropriate for big data called Tree-structured Parazen Estimator (TPE) within the hyperopt package. TPE starts learning good values for your hyperparameters (within a range we set) as it goes through multiple trials. The bayesian approach is helpful for big data tuning because we do not have the compute resources to do a comprehensive grid search.\n",
    "\n",
    "After we found our best hyperparameters for each estimator with cross-validation, in STEP 5, we prepared the full training dataset (2015-2018) and test set (2019) for our final model runs. Specifically, we complete the missing value imputation, normalization, and graph feature creation for both the training and test sets, appling the metadata from the 2015-2018 training dataset to the 2019 test set. Then, finally, in STEP 6, we do several more experiments on the full set of 2015-2018 training data to hone in our Final Model, using best model hyperparameters from STEP 4. The Final Model is the estimator that performs the best from STEP 6. In STEP 7, we use our Final Model to predict our flight delay outcome on the test set (2019). This gives us the final and best evaluation of our model, the f-beta score for the 2019 test set.\n",
    "\n",
    "**Pipeline Image**\n",
    "![Pipeline Image](https://i.imgur.com/wq62T0E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4cd67b0-17d3-42c8-9b9d-551e4146de69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## B. Families of Input Features\n",
    "We divide our input features into families based on the specific data processing steps we must apply to them. The feature families used in our hypertuning and final models are below. We originally had 19 features, but once we one-hot encode the categorical features, we have **81 input columns**.\n",
    "- **Numeric/Ratio (10 features)** are features that require a missing imputation strategy and normalization:  DISTANCE, ELEVATION, FE_PRIOR_DAILY_AVG_DEP_DELAY, FE_PRIOR_AVG_DURATION, FE_NUM_FLIGHT_SCHEDULED, DEP_DELAY_LAG, pagerank_origin, pagerank_dest, triangle_count_origin, triangle_count_origin\n",
    "- **Categorical (9 features)** are features that we must one-hot encode, including a separate category for missing: DAY_OF_WEEK, MONTH, YEAR, OP_UNIQUE_CARRIER, CRS_DEP_BUCKET origin_type, dest_type, is_holiday, is_holiday_adjacent, IS_FIRST_FLIGHT_OF_DAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe3d1fd-ded8-4fff-97d4-d4b833610b02",
     "showTitle": false,
     "title": ""
    },
    "id": "YXPyPNaAQXbC"
   },
   "source": [
    "## C. Experimental Modeling\n",
    "\n",
    "In the Experimental Modeling section, we describe the series of experiments we performed on the training dataset to arrive at our final trained model. We describe our Baseline, our Hyperparameter Tuning, and our final runs on the full 2015-2018 training dataset. In our final runs, we also experimented with an innovative technique to downsample the training set during our final runs on the 2015-2018 dataset, which boosted performance. Please see the Data Dictionary above for a full list of features we used in our Baseline vs the Hypertuning and full 2015-2018 training set runs.\n",
    "\n",
    "**Baseline.** We calculated our baseline with only 3 months of data from 2015 (with 3 cross-validation folds). Notably, the 3 months is a different time span than we used for hypertuning, so the baseline results are not apples-to-apples with the remaining cross-validation trials or final model runs. We considered two different estimators for our baseline model, a logistic regression and a random forest model, both with default parameters in the mLlib package. The baseline f-beta is a dismal 0.0 to 0.033, which is equivalent to the naive baseline that assumes all flights are on-time. A logistic model assumes that the target variable has a linear relationship with the feature variables, where a random forest model makes no such assumption. Logistic models are useful for its interpretability but more sophisticated models such as random forests and gradient-boosted trees tend to outperform the Logistic regression. Random forest is a bagging model that trains a lot of trees on samples of data (and also randomly samples features for each tree), aggregating the results. This approach can reduce the overfitting that may occur with a logistic regression. We expected the random forest to perform better than the logistic regression and also like that it does not assume a linear relationship between features and the outcome. We elected the random forest model as our baseline as we expect that the random forest model will far out-perform the logistic regression as we continue to refine our model and expand the time window of data.\n",
    "\n",
    "|Estimator   |Dataset      |Key Parameters | Runtime | F-Beta (beta=2) Metric| \n",
    "|------------|-------------|---------------|----------------------|----------------------|\n",
    "|Logistic Regression|Jan-Mar 2015 with 3 CV Folds|defaults: maxIter=100, regParam=0.0, elasticNetParam = 0.0| 1 min | 0.033                 |\n",
    "|Random Forest (BASELINE)|Jan-Mar 2015 with 3 CV Folds|defaults: maxDepth=5, numTrees=20| 1 min |0                     |\n",
    "\n",
    "**Hypertuning Experiments**. Hypertuning is the process of picking the parameters of your estimator that can optimize your f-beta score. For example, in tree-based estimators, a key parameter is the maximum depth of the tree. We did a series of experiments by hypertuning with the 2 most recent of our 5 Cross Validation folds that spanned part of 2017 and all of 2018 (200 days train, 92 days validation) on our estimators: random forest, LightGBM (a popular gradient boosted tree algorithm that is faster than XGBoost due to its leaf-wise (vertical) growth method, resulting in more loss reduction and higher accuracy on big data). While we originally intended to conduct the cross-validation on all 5 folds covering the full 2015-2018 period, we scaled down to 2 folds due to time-constraints. We used the hyperopt package with the Bayesian TPE algorithm. The bayesian algorithm learns the best parameters as it increases the number of trials. For example, we leveraged hyperopt’s capability to select from a uniform distribution with a defined minimum and maximum value. Since hyperopt “minimizes” your metric (f-beta), we minimized the negative f-beta score to get the appropriate result. We were constrained by time we only could try a limited number of hypertuning trials (8 random forest trials, 10 lightGBM trials, 2 neural network trials) and we focused on what we believe are the most impactful hyperparameters. The random forest trials and neural network runs took more time than the lightGBM trials. We believe we could do somewhat better if we had more time perform a more exhaustive hyperparameter search. Table 2 below shows the hyperparameter search space we used for each of our estimators and the Cross-Validation results.\n",
    "\n",
    "- For the **random forest estimator**, we tuned the most important parameters, number of trees and maximum depth of the trees. We found the best f-beta score with 140 trees, with a maximum depth of each tree of 12 features.\n",
    "\n",
    "- For **lightGBM** we tuned tree-specific parameters (e.g. maximum depth of the trees, minimum data in a leaf to control overfitting, subsample size), and regularization parameters (learning rate alpha) for xgboost, which can help reduce model complexity and enhance performance. We found the best f-beta score with max depth of 10 trees, minimum child weight of 2, minimum data in leaf of 1000, subsample of 0.8 and alpha rate 1. \n",
    " \n",
    "- For the **neural network**, we experimented with two different neural network architectures rather than a TPE hyperparameter search.  The neural network estimator is a classifier trainer based on the Multilayer Perceptron, with sigmoid activation functions at each layer. The input layer has 1 node per feature and the output layer has softmax with 2 nodes (i.e., binary classification). The first architecture layer specification is 81 (Input Layer) - 32 (Hidden Layer w/ Sigmoid Activation) - 2 (Output Layer w/ Softmax Activation). The second layer specification is 81 (Input Layer) - 8 (Hidden Layer #1 w/ Sigmoid Activation) – 4 –  (Hidden Layer #2 w/ Sigmoid Activation) - 2 (Output Layer w/ Softmax Activation). We found similar results for both models, but the latter performed slightly better.\n",
    "\n",
    " In summary, the neural network performed the best with a f-beta score of 43% followed by random forest with a f-beta of 40% (See Table 1). The lightGBM performed the worst, with a f-beta of 29%, so we dropped this estimator from the final model runs with the full 2015-2018 training data.\n",
    "\n",
    "**Table 2. Hypertuning Experiments (3 Hypertuning Experiments using Cross-Validation Folds)**\n",
    "|#           |Estimator    |Dataset        |Best Hyperparameters|Runtime of HyperTuning|Number of evaluation trials                                                |Best F-Beta (beta=2) Metric|\n",
    "|------------|-------------|---------------|--------------------|----------------------|---------------------------------------------------------------------------|---------------------------|\n",
    "|1           |Random Forest|2017-2018 (2-folds). train: 200 days, val: 92|maxDepth=5, numTrees=20 <br/><br/>**Search Space**: \"maxDepth\": hp.quniform(\"maxDepth\", 4, 12, 2), \"numTrees\": hp.quniform(\"numTrees\", 20, 200, 20)|8 hours               |8                                                                          |40.1%                     |\n",
    "|2           |LightGBM     |2017-2018 (2-folds). train: 200 days, val: 92|maxDepth=10 Min_child_weight=2 Min_data_in_leaf=1000 Subsample=0.8 reg_alpha=1maxDepth=10, min_child_weight=2, min_data_in_leaf=1000, subsample=0.8, reg_alpha=1 <br/><br> **Search Space**: \"maxDepth\": hp.quniform(\"maxDepth\", 2, 10, 1), 'max_depth':hp.quniform(\"max_depth\", 3,10,2), ‘min_child_weight':hp.quniform(\"min_child_weight\", 1,6,2), 'min_data_in_leaf':hp.quniform(\"min_data_in_leaf\", 100, 1000, 200), 'subsample':hp.choice(\"subsample\", [i/10.0 for i in range(6,10)]), 'reg_alpha':hp.choice(\"reg_alpha\", [1e-5, 1e-2, 0.1, 1, 100])|3 hours               |10                                                                         |28.7%                     |\n",
    "|3           |Neural Network|2017-2018 (2-folds). train: 200 days, val: 92|layers=[81, 8, 4, 2], maxIter=100, blockSize=128, stepSize=0.03|3 hours               |2                                                                          |**43.4%**                 |\n",
    "\n",
    "\n",
    "**Final Runs on Full 2015-2018 Training Set.** We ran the next set of experiments on the full 2015-2018 training dataset. We present both the f-beta and the AUC Metric for greater context (AUC = Area Under the Curve, where the x-axis is the false positive rate and the y-axis the true positive rate). The AUC metric is calculated using the predicted probabilities of each flight and represents how well the model can distinguish between outcome classes, ranging from 50% to 100%, with 50% as good as random chance and 100% as a perfect score. For experiments, 1-2 in Table 3, we applied the best hyperparameters from the random forest and neural network estimators to train our full 2015-2018 training set. In experiments 4 and 5, we applied an innovative, statistical downsampling technique to the training set, again on both a random forest and neural network model. Downsampling allows us to create a balanced training set with an equal number of delayed and on-time flights (see Table 3). We retain all delayed flights and randomly down-sampled the on-time ones to achieve this balance. As a technical note related to data leakage, it's important to note that this down-sampling is limited to the training data only, not affecting the test dataset. The approach aims to ensure that our models learn to discriminate between classes using an equal number of examples, leading to better generalization. When deployed in production, the model will make predictions on a more significant number of on-time flights than delayed ones, but its training on a balanced dataset enhances its overall performance. The distribution of the downsampled dataset is shown in Table 4. \n",
    "\n",
    "Experiment 5, a neural network with 2 hidden layers trained on a downsampled 2015-2018 training set, has the highest f-beta score, with a f-beta score of 63.1%. The corresponding AUC is also one of the highest, at 71.8%.\n",
    "\n",
    "**Table 3: Experiments on 2015-2018 Training Dataset (5 Experiments)**\n",
    "|#|Estimator   |Dataset      |Hyperparameters|Runtime|F-Beta (beta=2) Metric|AUC Metric|\n",
    "|-|------------|-------------|---------------|----------------------|----------|-------|\n",
    "|1|Neural Network 1|2015-2018 Train|layers=[81, 8, 4, 2], maxIter=100, blockSize=128, stepSize=0.03| 20 min| 40.5%                |66.9%    |\n",
    "|2|Neural Network 2|2015-2018 Train|layers=[81, 32, 16, 4, 2], maxIter=100, blockSize=128, stepSize=0.03| 50 min |40.3%                |67.0    |\n",
    "|3|Random Forest 1|2015-2018 Train|maxDepth=5, numTrees=20| 50 min | 40.5%                |50.3%    |\n",
    "|4|Random Forest 2 |2015-2018 Downsampled Training Set|maxDepth=5, numTrees=20| 35 min |60.4%        |72.0|\n",
    "|5|Neural Network 3|2015-2018 Downsampled Training Set|layers=[81, 8, 4, 2], maxIter=100, blockSize=128, stepSize=0.03| 21 min | **63.1%**     |71.8%|\n",
    "\n",
    "**Table 4. Distribution on the Outcome Variable \"DEP_DEL15\" Before and After Downsampling (Canceled Flights Removed)**\n",
    "|Label    |Training Dataset Row Count (original) <br/> 2015-2018|Training Dataset After Downsampling Row Count <br/> 2015-2018|Test Dataset Row Count<br/> 2019|\n",
    "|---------|-----------------------------------------------------|-------------------------------------------------------------|--------------------------------|\n",
    "|No Delay |19,607,943                                           |4,312,859                                                    |5,905,453                       |\n",
    "|Delay    |4,312,859                                            |4,312,859                                                    |1,353,702                       |\n",
    "|**Total**    |**23,920,802**                                           |**8,625,718**                                                    |**7,259,155**                       |\n",
    "\n",
    "\n",
    "**Loss Functions for Estimators.** For completeness, we provide the loss functions for our estimators we considered are below.\n",
    "- **Logistic Regression, Gradient Descent, and Neural Network Backpropagation**: The logistic loss function is used for our standard logistic regression model as well as for the optimization of the weights and biases during back-prop learning.\n",
    "\n",
    "$$ \\text{LogisticLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right] $$\n",
    "\n",
    "Where:\n",
    "$$ N\\text{: the number of flights} $$\n",
    "$$ y_i\\text{: the true label (1 if flight was actually delayed)} $$\n",
    "$$ p_i\\text{: the predicted probability that the i-th flight would be delayed} $$\n",
    "\n",
    "\n",
    "- **Random Forest and XGBoost**: For our Random Forest and XGBoost models, we employ the entropy-based information gain loss function with each split in each tree.\n",
    "$$ H(X) = I_E(p_1, p_2, ..., p_J) = - (p_1 \\cdot \\log_2(p_1) + p_2 \\cdot \\log_2(p_2)) $$\n",
    "\n",
    "Where:\n",
    "$$ J\\text{: the number of classes (2 for binary classification)} $$\n",
    "$$ p_i\\text{: the proportion of samples belonging to class i in a particular node} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b78ac9-35f4-4d71-8998-2885f58c98c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# VI. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0f25de-3968-49c3-89af-0c8deb9592d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Overall, we see that we have made marked improvement from our baseline (f-beta = 0.0) using 3 months of training data to our hyperparameter tuning with cross-fold validation on training data from 2017 and 2018 (f-beta=43.4%) to our final model runs on the full downsampled 2015-2018 training data (f-beta = 63.1%). The biggest takeaway during our 5 experiments in Table 2 is the power of downsampling the majority class in the training set to address the challenges associated with predicting a rare outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c1e683-94af-4336-a5c9-e0cb3040a3c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A. Heldout 2019 Test Set\n",
    "**2019 Holdout Results.** Using the best model from Table 3 (Experiment #5), we evaluated our holdout unseen test set with the full year of 2019 flight data. Our final f-beta metric on the unseen 2019 test dataset is 54.4%, as shown in Table 5 below. This performance drop from the training set to unseen data is expected. Our model is fit to the training data from 2015-2018 and therefore will not generalize as well to an unseen holdout dataset from 2019. Next, we present a gap analysis of the predictions on the 2019 Holdout Dataset to highlight where we are predicting well and where we are predicting less well, helping to guide future feature engineering work.\n",
    "\n",
    "**Table 5: 2019 Holdout Data Result**\n",
    "|Estimator   |Dataset      |Hyperparameters|F-Beta (beta=2) Metric|AUC Metric|\n",
    "|------------|-------------|---------------|----------------------|----------|\n",
    "|Neural Network 1|2019 Holdout|layers=[81, 8, 4, 2], maxIter=100, blockSize=128, stepSize=0.03| 54.4% | 71.0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6de4b381-ad0b-49d6-a32d-e99d8ff43bd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### B. Gap Analysis: Analysis of False Negatives\n",
    "\n",
    "In this section, we conduct a post-mortem gap analysis on the prediciton on the 2019 heldout, test set to examine patterns in our prediction errors to gain insight into where this model could be improved. To examine the gap between the model's expected performance and the actual results, we will evaluate how the f-beta score differs across the different airlines as well as over different times throughout the week. Here we just examine the holdout test data covering 2019, because we are most interested in the error patterns that arise with data the model has not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed742e0-2bd8-4609-af11-2950e3877738",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Model Performance by Airline**\n",
    "\n",
    "First we look at how consistently our model performs across all airlines, big and small. Do we see a pattern that could indicate a systemic issue we might flag for future work?\n",
    "\n",
    "![Model Performance by Airline](https://i.imgur.com/Btr1TIq.png)\n",
    "\n",
    "**Insights**\n",
    "* From this plot, we can see that our model handles small airports well, except for Hawaii Air (HA) and Alaska Air (AS) who perform many more long-haul flights considering that they are headquartered outside of the continental United States. \n",
    "* Surprisingly, our model significantly underperforms on the major airlines United (UA), American (AA), and Delta (DL). This observation suggests the need for a closer examination of the underlying factors affecting the prediction accuracy for these airlines.\n",
    "* Southwest Airlines (WN) emerges as a carrier that our model predicts with relative ease, potentially due to distinctive operational characteristics or patterns that align well with our model's features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05e73e32-88b1-484d-81e8-ba37bb386003",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Model Performance by Time of Week**\n",
    "\n",
    "Let's delve into the consistency of the model's performance across the different times of the week. In the below chart, we round down the departure time to the hour. \n",
    "\n",
    "![Heatmap of F2 Scores by Day of Week and Hour](https://i.imgur.com/JFcIrox.png)\n",
    "\n",
    "**Insights**\n",
    "* The model's performance is notably weak during times when flights are infrequent, particularly flights scheduled before 8:00 a.m.\n",
    "* As the day progresses and more flights take place, the model's performance improves significantly.\n",
    "* The model demonstrates better performance on weekdays compared to weekends.\n",
    "* The fact that the model performs better where there are more consecutively booked flights is promising - those are the flights airlines struggle the most with keeping on-time due to congestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b344abb0-e548-481b-ab98-9d986146e1e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Gap Analysis Recommendations for Further Improvement**\n",
    "\n",
    "Based on the above findings, we recommend the following feature engineering work done to further improve the model: \n",
    "\n",
    "* **Cross-Oceanic Flight Indicator**: Include a feature that distinguishes airlines that primarily carry out cross-oceanic flights. Currently, we account for the distance of the flight but there could also be a factor when traveling outside the contiguous 48 US states. \n",
    "* **Hour-of-Day Segmentation**: Include a feature that breaks out hour of the day similar to quartiles with differently-lengthed time spans. For example, there may be as many flights departing between 12am and 10am as there are departing between 10am and noon or between 5pm and 6pm. \n",
    "* **Weekend Indicator**: Provide a binary variable distinguishing the weekend from the weekdays to directly capture the difference in demands. Perhaps this difference stems from business vs. leisure travel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84b7d861-7b93-40bb-ac48-99e72ea716bb",
     "showTitle": false,
     "title": ""
    },
    "id": "QK3o7Yz1LVpc"
   },
   "source": [
    "# VII. Conclusions and Next Steps\n",
    "\n",
    "Our goal was to create a machine learning classifier that reliably predicts whether a flight will be delayed 2 hours prior to its departure. This tool will help the airline industry improve their daily operations and increase their profit. We focused on a f-beta metric that errors on the side of predicting more flight delays and being wrong, with a goal of achieving at least the 80% benchmark. The predictive power of our model, with a f-beta score of 54.4% on the 2019 holdout test set, is quite low, and so we will continue to work to refine this tool. Our best estimator was a neural network with two hidden layers and the random forest estimator came in at a close second. We believe foremost we should improve the set of features, and secondly, continue to look for better estimators, including ensemble models, and hyperparameters to make smaller improvements. We will also look to feature selection methods such as Shap values to further refine our feature set. We found great power in random downsampling the training set to fit our model and can also continue to try other non-random downsampling approaches such as downsampling at the day- or week-level. We built a robust data pipeline that can rapidly integrate new features into the model and minimize data leakage.\n",
    "\n",
    "Our gap analysis showed that we predicted delays for some airline carriers better than others and we also seen variation by time of day and day of the week. For certain, we are not fully capturing the myriad of factors that contribute to a flight's delay. We need more predictive features that can capture daily operational challenges of the airport environment that could lead to delays such as staff scheduling issues and airplane maintenance schedules. We plan to improve this model by doing more feature engineering and conducting hyperparameter tuning over more trials. There are many opportunities for more features, such as those identified in our gap analysis, creating daily or weekly seasonal trend features, an inclement weather feature, and interaction terms between month and holiday (MONTH x HOLIDAY) or OP_UNIQUE_CARRIER*DAY_OF_WEEK. We also might pull in more datasets that represent the more event-based features like the Superbowl, airport maintenance, and airline reputation.\n",
    "\n",
    "**Scalability concerns/Challenges/Learnings when working with Big Time-Series Data**. This project was an excellent introduction to machine learning approaches with time-series, big data. We learned that we must primarily work with a small subset of the data for hyperparameter tuning as tuning a 4 year training set with cross-validation folds can easily take most of a day to run. However, doing some of the training experiments on a different subset of data than you use for your final model runs presents some new challenges as well. We were unsure how well our cross-validation results on our subset will generalize to the broader training set (although in our case the results were the same across both datasets). The size of the data also affects the complexity of the model we would attempt. For example, we attempted only a handful of modest neural network architectures. Simillary, we performed only a limited amount of hyperparameter tuning due to the compute constraints of big data within the short deadline of this project. Our bayesian hyper tuning approach that learns how to pick better parameters is an exciting approach to optimize our model. We also faced productivity constraints as a team due to the cluster size constraints in the cloud. This was a group project and we also had to sequence our work so that only one team member was working on the cloud data cluster at a time once we were working with the full 5-year dataset. Unfortunately, due to additional processing time needed to complete the main model runs, we were not able to implement the secondary Prophet model that we intended to ensemble. But it was tested on smaller data sets and trained successfully on the full final dataset.\n",
    "\n",
    "**Innovations**. We offered several innovative elements to our approach, namely trying a statistical downsampling procedure to address class imbalance that dramatically improved our model's performance. We also created sophisticated graph features (i.e., pagerank and triangle count) to represent the role of each airport in the network and tried using a special LightGBM package that provides speedy results for gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4842cf3f-0896-46e9-8e41-b336bf061bdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FINAL REPORT FLIGHT DELAYS",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39603e67-4d40-44e2-9fe7-325cc75f58d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Hypertuning with Cross-Validation Folds: Neural Networks\n",
    "This code estimates neural network models and prints out our evaluation metric (f-beta, where beta=2). The code is working with the cleaned cross-validation folds created from the 2015-2018 dataset. We do both a baseline assessment accross 3-months of data and a full hypertuning with the 2015-2018 cross-validation folds. In the future, we plan to also try ensemble models. We adopted a bayesian hypertuning strategy appropriate for big data called Tree-structured Parazen Estimator (TPE) within the hyperopt package. TPE starts learning good values for your hyperparameters (within a range we set) as it goes through multiple trials. The bayesian approach is helpful for big data tuning because we do not have the compute resources to do a comprehensive grid search.\n",
    "\n",
    "![Pipeline Image](https://i.imgur.com/wq62T0E.png)\n",
    "\n",
    "### Project Description\n",
    "This is a group project conducted for course w261: Machine Learning at Scale at the University of California Berkeley in Summer 2023. This project develops a machine learning model that predicts flight delays based on historical flight, airport station, and weather data spanning five years from 2015-2019 in the United States.\n",
    "\n",
    "###Group members\n",
    "Jessica Stockham, Chase Madison, Kisha Kim, Eric Danforth\n",
    "\n",
    "Citation: Code written by Chase Madison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6d7ad5-1861-421c-8864-51818ea1ec9f",
     "showTitle": true,
     "title": "Import Packages"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from datetime import datetime, timedelta, date\n",
    "import holidays\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.sql.functions import udf, col,isnan,when,count\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,StandardScaler, Imputer, Bucketizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#from sparkxgb.xgboost import XGBoostClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from hyperopt import fmin, tpe, Trials, SparkTrials, hp, space_eval\n",
    "import mlflow\n",
    "#import mlfow.spark\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e06db7-f12b-4f0d-bfad-a4dac06916da",
     "showTitle": true,
     "title": "Access to Team Blob Storage Container"
    }
   },
   "outputs": [],
   "source": [
    "## Place this cell in any team notebook that needs access to the team cloud storage\n",
    "mids261_mount_path = '/mnt/mids-w261'  # 261 course blob storage is mounted here\n",
    "secret_scope = 'sec5-team1-scope'  # Name of the secret scope Chase created in Databricks CLI\n",
    "secret_key = 'sec5-team1-key'  # Name of the secret key Chase created in Databricks CLI\n",
    "storage_account = 'sec5team1storage'  # Name of the Azure Storage Account Chase created\n",
    "blob_container = 'sec5-team1-container'  # Name of the container Chase created in Azure Storage Account\n",
    "team_blob_url = f'wasbs://{blob_container}@{storage_account}.blob.core.windows.net'  # Points to the root of your team storage bucket\n",
    "spark.conf.set(  # SAS Token: Grant the team limited access to Azure Storage resources\n",
    "  f'fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net',\n",
    "  dbutils.secrets.get(scope=secret_scope, key=secret_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b673a5d-444a-44e0-9057-9278f031e509",
     "showTitle": true,
     "title": "Functions to load data and conduct training on CV folds"
    }
   },
   "outputs": [],
   "source": [
    "# LOAD CLEANED CV FOLDS FROM BLOB\n",
    "\n",
    "def load_folds_from_blob_and_cache(blob_url, fold_name):\n",
    "    folds = list()\n",
    "    DEFAULT_PARTITION_COUNT = 50\n",
    "\n",
    "    # Compute the fold count\n",
    "    files = dbutils.fs.ls(f\"{blob_url}/{fold_name}\")\n",
    "    fold_names = sorted([f.name for f in files if f.name.startswith(\"train\")])\n",
    "    match = re.match(r\"train_(\\d+)_df\", fold_names[-1])\n",
    "    fold_count = int(match.group(1)) + 1\n",
    "    print(f\"Loading {fold_count} folds...\")\n",
    "\n",
    "    # Load folds\n",
    "    for i in range(fold_count):\n",
    "        train_df = (\n",
    "            spark.read.parquet(f\"{blob_url}/{fold_name}/train_{i}_df\")\n",
    "            .repartition(DEFAULT_PARTITION_COUNT)\n",
    "            .cache()\n",
    "        )\n",
    "        val_df = (\n",
    "            spark.read.parquet(f\"{blob_url}/{fold_name}/val_{i}_df\")\n",
    "            .repartition(DEFAULT_PARTITION_COUNT)\n",
    "            .cache()\n",
    "        )\n",
    "        folds.append((train_df, val_df))\n",
    "    return folds\n",
    "\n",
    "\n",
    "# Train Cross Validation Folds\n",
    "def trainPredictEval(estimator):  \n",
    "\n",
    "    \"\"\"\n",
    "    Get validation fscore across all folds. Function is called by objective_function_rf()\n",
    "\n",
    "    Parameters:\n",
    "        estimator: machine learning model defined in objective_function_rf()\n",
    "    \n",
    "    returns:\n",
    "        average validation fscore accross all folds\n",
    "    \"\"\"\n",
    "    from statistics import mean \n",
    "\n",
    "    metricsList = []\n",
    "\n",
    "    # Load folds data\n",
    "    for i, (train_df, val_df) in enumerate(folds):\n",
    "\n",
    "        print(f'CV FOLD START: {i}: {datetime.now()}')\n",
    "        \n",
    "        # Train\n",
    "        model = estimator.fit(train_df)\n",
    "\n",
    "        print(f'Model built: {i}: {datetime.now()}')\n",
    "        \n",
    "        pred = model.transform(val_df).cache()\n",
    "        \n",
    "        print(f'Prediction Validation Set: {i}: {datetime.now()}')\n",
    "            \n",
    "        # Compute Metrics\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"fMeasureByLabel\", beta=2.0, metricLabel=1.0)\n",
    "        fmeasure = evaluator.evaluate(pred, {evaluator.metricLabel: 1.0})\n",
    "        print(fmeasure)\n",
    "\n",
    "        metricsList.append(fmeasure)\n",
    "        print(f'fold fscore: {fmeasure}')\n",
    "\n",
    "        pred.unpersist()\n",
    "\n",
    "    avgFscore = mean(metricsList)\n",
    "    print(f'average fscore accross fold: {avgFscore}')\n",
    "\n",
    "    # mlflow logging\n",
    "    mlflow.log_metric(\"f2_score\", (-1)*avgFscore)\n",
    "\n",
    "    # negate fscore becuase hyperopt minimizes\n",
    "    return (-1)*avgFscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0ff7399-894c-4f17-b03b-a604b181b353",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Hypertuning Cross-Validation with 60 Month Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2185216d-7460-45a0-aaa5-865480ed68c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 5 folds...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(DataFrame[features: vector, label: double],\n",
       "  DataFrame[features: vector, label: double]),\n",
       " (DataFrame[features: vector, label: double],\n",
       "  DataFrame[features: vector, label: double])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### LOAD 60 MONTH DATASET ##########\n",
    "\n",
    "# 60 MONTH DATA - 5 folds - (READY TO USE)\n",
    "# RUN THIS CODE BELOW TO PULL IN CV FOLDS IN VARIABLE \"folds\" for 60 month (only brings in 2015-2018 as training set)\n",
    "timeInterval = '60mo'\n",
    "fold_name = \"folds\" + timeInterval\n",
    "folds = load_folds_from_blob_and_cache(team_blob_url, fold_name)\n",
    "\n",
    "# Filter to the most recent 2 folds (2017 and 2018)\n",
    "fold_small = folds[3:5]\n",
    "\n",
    "# Create folds_slim that excludes ORIGIN_hot and DEST_hot (representing about 600 columns)\n",
    "folds_slim = []\n",
    "\n",
    "for i, (train_df, val_df) in enumerate(fold_small):\n",
    "\n",
    "    train_df_new = train_df.drop(\"features\")\n",
    "    val_df_new = val_df.drop(\"features\")\n",
    "    \n",
    "    features_all = ['IS_FIRST_FLIGHT_OF_DAY_double_hot',\n",
    "    'is_holiday_adjacent_double_hot',\n",
    "    'OP_UNIQUE_CARRIER_hot',\n",
    "    'is_holiday_double_hot',\n",
    "    'CRS_DEP_BUCKET_hot',\n",
    "    'DAY_OF_WEEK_hot',\n",
    "    'origin_type_hot',\n",
    "    'dest_type_hot',\n",
    "    'MONTH_hot',\n",
    "    'YEAR_hot'] + ['scaled_numeric']\n",
    "\n",
    "    #print(f'features_all: {features_all}')\n",
    "    assembler = VectorAssembler(inputCols=features_all, outputCol=\"features\")\n",
    "\n",
    "    train_df_slim = assembler.transform(train_df_new)\n",
    "    val_df_slim = assembler.transform(val_df_new)\n",
    "\n",
    "    train_df_slim = train_df_slim.select(['features', 'label'])\n",
    "    val_df_slim = val_df_slim.select(['features', 'label'])\n",
    "\n",
    "    folds_slim.append((train_df_slim, val_df_slim))\n",
    "\n",
    "folds_slim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71fce0a-9512-476e-89dd-8de65a7915dd",
     "showTitle": true,
     "title": "Inspect the Schema"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- label: double (nullable = true)\n |-- DISTANCE: double (nullable = true)\n |-- ELEVATION: double (nullable = true)\n |-- FE_PRIOR_DAILY_AVG_DEP_DELAY: double (nullable = true)\n |-- FE_PRIOR_AVG_DURATION: double (nullable = true)\n |-- FE_AVG_DURATION: double (nullable = true)\n |-- FE_NUM_FLIGHT_SCHEDULED: long (nullable = true)\n |-- DEP_DELAY_LAG: double (nullable = true)\n |-- DAY_OF_WEEK: integer (nullable = true)\n |-- MONTH: integer (nullable = true)\n |-- YEAR: integer (nullable = true)\n |-- OP_UNIQUE_CARRIER: string (nullable = true)\n |-- origin_type: string (nullable = true)\n |-- dest_type: string (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- DEST: string (nullable = true)\n |-- is_holiday_double: double (nullable = true)\n |-- is_holiday_adjacent_double: double (nullable = true)\n |-- IS_FIRST_FLIGHT_OF_DAY_double: double (nullable = true)\n |-- CRS_DEP_TIME: integer (nullable = true)\n |-- DATE: timestamp (nullable = true)\n |-- FL_DATE: date (nullable = true)\n |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n |-- DEP_DELAY: double (nullable = true)\n |-- AIR_TIME: double (nullable = true)\n |-- DEP_TIME_BLK: string (nullable = true)\n |-- origin_iata_code: string (nullable = true)\n |-- dest_iata_code: string (nullable = true)\n |-- TAIL_NUM: string (nullable = true)\n |-- sched_depart_date_time_UTC: timestamp (nullable = true)\n |-- CRS_DEP_BUCKET: double (nullable = true)\n |-- DAY_OF_WEEK_ix: double (nullable = true)\n |-- MONTH_ix: double (nullable = true)\n |-- YEAR_ix: double (nullable = true)\n |-- OP_UNIQUE_CARRIER_ix: double (nullable = true)\n |-- origin_type_ix: double (nullable = true)\n |-- dest_type_ix: double (nullable = true)\n |-- ORIGIN_ix: double (nullable = true)\n |-- DEST_ix: double (nullable = true)\n |-- is_holiday_double_ix: double (nullable = true)\n |-- is_holiday_adjacent_double_ix: double (nullable = true)\n |-- IS_FIRST_FLIGHT_OF_DAY_double_ix: double (nullable = true)\n |-- CRS_DEP_TIME_ix: double (nullable = true)\n |-- DAY_OF_WEEK_hot: vector (nullable = true)\n |-- MONTH_hot: vector (nullable = true)\n |-- YEAR_hot: vector (nullable = true)\n |-- OP_UNIQUE_CARRIER_hot: vector (nullable = true)\n |-- origin_type_hot: vector (nullable = true)\n |-- dest_type_hot: vector (nullable = true)\n |-- ORIGIN_hot: vector (nullable = true)\n |-- DEST_hot: vector (nullable = true)\n |-- is_holiday_double_hot: vector (nullable = true)\n |-- is_holiday_adjacent_double_hot: vector (nullable = true)\n |-- IS_FIRST_FLIGHT_OF_DAY_double_hot: vector (nullable = true)\n |-- CRS_DEP_TIME_hot: vector (nullable = true)\n |-- pagerank_origin: double (nullable = true)\n |-- triangle_count_origin: long (nullable = true)\n |-- pagerank_dest: double (nullable = true)\n |-- triangle_count_dest: long (nullable = true)\n |-- DISTANCE_imputed: double (nullable = true)\n |-- ELEVATION_imputed: double (nullable = true)\n |-- FE_PRIOR_DAILY_AVG_DEP_DELAY_imputed: double (nullable = true)\n |-- FE_PRIOR_AVG_DURATION_imputed: double (nullable = true)\n |-- FE_AVG_DURATION_imputed: double (nullable = true)\n |-- FE_NUM_FLIGHT_SCHEDULED_imputed: long (nullable = true)\n |-- DEP_DELAY_LAG_imputed: double (nullable = true)\n |-- pagerank_origin_imputed: double (nullable = true)\n |-- pagerank_dest_imputed: double (nullable = true)\n |-- triangle_count_origin_imputed: long (nullable = true)\n |-- triangle_count_dest_imputed: long (nullable = true)\n |-- vectorized_numeric_features: vector (nullable = true)\n |-- scaled_numeric: vector (nullable = true)\n |-- features: vector (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Schema has \"features, label\" + individual features + intermediate features used for processing. \n",
    "# Jess changed this. Before just kept \"features, label\" but realized this gives us more flexibility.\n",
    "# Could change \"features\" input on the fly if you wanted.\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7828a7-09b7-4bd7-b16b-a8dd021e69ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLP: Multilayer Perceptron Neural Networks with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319c907d-d354-49aa-b22d-3924f4d29cae",
     "showTitle": true,
     "title": "1. Define Objective Function Specific to Algorithm "
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "# CANT REMEMBER IF THESE IMPORTS ARE NEEDED, BUT JUST DELETE IF THEY DONT THROW A DEPENDENCY ERROR\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "# GRID SEARCH MLP\n",
    "def objective_function_mlp(params):\n",
    "\n",
    "    \"\"\"\n",
    "    Define estimator\n",
    "\n",
    "    Parameters:\n",
    "        params: default in hyperopt. Do not change.\n",
    "    \n",
    "    returns:\n",
    "        trainPredictEval(estimator) function\n",
    "    \"\"\"\n",
    "\n",
    "    # set hyperparameters we want to tune\n",
    "    architecture = params[\"architecture\"]\n",
    "\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        # Train\n",
    "        estimator = MultilayerPerceptronClassifier(\n",
    "            layers=architecture,  # The architecture of the neural network\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        return trainPredictEval(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6f511f-38a3-4aea-bc6d-c6ec0827c476",
     "showTitle": true,
     "title": "2. Define the Search Space"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# Get the number of features in the training data\n",
    "\n",
    "# Calculate the maximum number of features across all folds\n",
    "num_features = len([x[\"name\"] for x in sorted(folds[0][0].schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"binary\"] + folds[0][0].schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"], key=lambda x: x[\"idx\"])])\n",
    "print(num_features)\n",
    "\n",
    "# Defining the layer node sizes for the neural network\n",
    "nn_architecture = {\n",
    "    #'one_layer': [num_features, 32, 2],     # One hidden layer with many nodes\n",
    "    'two_layers': [num_features, 8, 4, 2]  # Two hidden layers with fewer nodes\n",
    "}\n",
    "search_space_mlp = {\n",
    "    \"architecture\": hp.choice(\"architecture\", [network for network in nn_architecture.values()]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7521677f-8893-435c-a47f-0ffe2c6b1c0f",
     "showTitle": true,
     "title": "3. Run the Trials"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Start: 2023-08-08 18:27:06.222221\n\r  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rCV FOLD START: 0: 2023-08-08 18:27:07.040570\n\r  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 18:27:07 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \rModel built: 0: 2023-08-08 19:06:48.133606\n\r  0%|          | 0/2 [39:41<?, ?trial/s, best loss=?]\r                                                     \rPrediction Validation Set: 0: 2023-08-08 19:06:48.227344\n\r  0%|          | 0/2 [39:41<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 19:08:24 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \r0.4600264477658919\n\r  0%|          | 0/2 [41:23<?, ?trial/s, best loss=?]\r                                                     \rfold fscore: 0.4600264477658919\n\r  0%|          | 0/2 [41:23<?, ?trial/s, best loss=?]\r                                                     \rCV FOLD START: 1: 2023-08-08 19:08:29.600001\n\r  0%|          | 0/2 [41:23<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 19:08:29 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \rModel built: 1: 2023-08-08 20:05:11.669153\n\r  0%|          | 0/2 [1:38:05<?, ?trial/s, best loss=?]\r                                                       \rPrediction Validation Set: 1: 2023-08-08 20:05:11.759502\n\r  0%|          | 0/2 [1:38:05<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:07:04 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                       \r0.40756826905311866\n\r  0%|          | 0/2 [1:40:03<?, ?trial/s, best loss=?]\r                                                       \rfold fscore: 0.40756826905311866\n\r  0%|          | 0/2 [1:40:03<?, ?trial/s, best loss=?]\r                                                       \raverage fscore accross fold: 0.4337973584095053\n\r  0%|          | 0/2 [1:40:03<?, ?trial/s, best loss=?]\r 50%|█████     | 1/2 [1:40:03<1:40:03, 6003.26s/trial, best loss: -0.4337973584095053]\r                                                                                      \rCV FOLD START: 0: 2023-08-08 20:07:10.175875\n\r 50%|█████     | 1/2 [1:40:03<1:40:03, 6003.26s/trial, best loss: -0.4337973584095053]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:07:10 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN THIS: HYPERTUNING: NEURAL NETWORK 1\n",
    "\n",
    "print(f'Job Start: {datetime.now()}')\n",
    "\n",
    "# End prior mlfow run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Keep logging off during hypertuning\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "\n",
    "num_evals = 2\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparam_mlp = fmin(\n",
    "    fn=objective_function_mlp,\n",
    "    space=search_space_mlp,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=num_evals,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(42)\n",
    ")\n",
    "\n",
    "# BEST PARAMETERS\n",
    "best_params = space_eval(search_space_mlp, best_hyperparam_mlp)\n",
    "print(f'best parameters: {best_params}')\n",
    "\n",
    "# LOG IT\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"CV_2folds_fscore_nn\", trials.best_trial['result']['loss'])\n",
    "\n",
    "# End prior mlfow run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e5ea70-18e9-4cd2-aad0-dea037f5c33b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Start: 2023-08-08 20:34:46.074184\n\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rCV FOLD START: 0: 2023-08-08 20:34:46.876072\n\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 20:34:47 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \rModel built: 0: 2023-08-08 21:05:20.077972\n\r  0%|          | 0/1 [30:33<?, ?trial/s, best loss=?]\r                                                     \rPrediction Validation Set: 0: 2023-08-08 21:05:20.176718\n\r  0%|          | 0/1 [30:34<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:06:19 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \r0.41530879029981377\n\r  0%|          | 0/1 [31:38<?, ?trial/s, best loss=?]\r                                                     \rfold fscore: 0.41530879029981377\n\r  0%|          | 0/1 [31:38<?, ?trial/s, best loss=?]\r                                                     \rCV FOLD START: 1: 2023-08-08 21:06:25.126077\n\r  0%|          | 0/1 [31:38<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:06:25 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                     \rModel built: 1: 2023-08-08 21:46:37.434522\n\r  0%|          | 0/1 [1:11:51<?, ?trial/s, best loss=?]\r                                                       \rPrediction Validation Set: 1: 2023-08-08 21:46:37.623358\n\r  0%|          | 0/1 [1:11:51<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/08 21:48:25 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r                                                       \r0.4055715091543289\n\r  0%|          | 0/1 [1:13:44<?, ?trial/s, best loss=?]\r                                                       \rfold fscore: 0.4055715091543289\n\r  0%|          | 0/1 [1:13:44<?, ?trial/s, best loss=?]\r                                                       \raverage fscore accross fold: 0.4104401497270713\n\r  0%|          | 0/1 [1:13:44<?, ?trial/s, best loss=?]\r100%|██████████| 1/1 [1:13:45<00:00, 4425.02s/trial, best loss: -0.4104401497270713]\r100%|██████████| 1/1 [1:13:45<00:00, 4425.03s/trial, best loss: -0.4104401497270713]\nbest parameters: {'architecture': (81, 8, 4, 2)}\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS: HYPERTUNING: NERUAL NETWORK 2\n",
    "\n",
    "print(f'Job Start: {datetime.now()}')\n",
    "\n",
    "# End prior mlfow run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Keep logging off during hypertuning\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "\n",
    "num_evals = 1\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparam_mlp = fmin(\n",
    "    fn=objective_function_mlp,\n",
    "    space=search_space_mlp,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=num_evals,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(42)\n",
    ")\n",
    "\n",
    "# BEST PARAMETERS\n",
    "best_params = space_eval(search_space_mlp, best_hyperparam_mlp)\n",
    "print(f'best parameters: {best_params}')\n",
    "\n",
    "# LOG IT\n",
    "# Just logging the artifacts, not the data, to save compute time\n",
    "# Need to rename your experiment\n",
    "\n",
    "# Log best CV parameters and fscore\n",
    "# experiment_name = \"phase3_cv_rf_CV\"\n",
    "# experiment_id = mlflow.create_experiment(experiment_name)\n",
    "# with mlflow.start_run(experiment_id=experiment_id):\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"CV_2folds_fscore_nn\", trials.best_trial['result']['loss'])\n",
    "\n",
    "# End prior mlfow run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hyperparameter Tuning with CV Folds: Neural Networks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

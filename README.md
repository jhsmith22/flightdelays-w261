### Context
This is a group project conducted for course w261: Machine Learning at Scale at the University of California Berkeley in Summer 2023. This project develops a machine learning model that predicts flight delays based on historical flight, airport station, and weather data spanning five years from 2015-2019 in the United States. We use DataBricks/pyspark in an Azure cloud computing environment.

### Group members
Jessica Stockham, Chase Madison, Kisha Kim, Eric Danforth

The code has citations throughout to idenify which group member authored each part of code.

### Abstract
This project develops a machine learning model that predicts flight delays based on historical flight, airport station, and weather data spanning five years from 2015-2019 in the United States.  A flight delay is a flight that departs at least 15 minutes past its scheduled time, per the FAA definition. We evaluate the usefulness of our model with a metric that is appropriate for the nature of the data and the business context. Flight delays are relatively rare (about 20 percent of flights) which suggests a focus on either precision or recall. Specifically, we use the f-beta evaluation metric that strikes a balance between recall and precision, but places greater weight on recall to score our various predictive models.  We prioritize recall, which is the number of flights we correctly predicted as delayed divided by all the flights that were truly delayed. This biases us to train a model that errors on the side of predicting more flight delays and being wrong because we do not want to miss any potential delays. It is easier for airlines and customers to recover logistically and reputationally from warning customers there may be a delay and being wrong. This is in contrast to using a more conservative metric that prioritizes precision that only announces a flight delay if we are extremely sure it will happen. A f-beta score ranges from 0 to 100% and an ideal f-beta score is 100%, but a good f-beta score would be between 80% and 100%.

We use data from 2015-2018 to build a model so we can predict flight delays on an unseen dataset of all flights in 2019. This is a big-data problem as the dataset has approximately 31 million rows. We process the data on a distributed cloud computing environment. We built models with 81 features (numeric and categorical) across multiple estimators using data from 2015-2018: logistic regression, random forest, a variant of gradient boosted trees, and neural networks. We also applied innovative approaches such as downsampling the majority class to address the class inbalance and engineering sophisticated graph-based features such as pagerank and triangle count. Our data pipeline is carefully designed to avoid data leakage that would harm the predictive power of the model. Data leakage is a common hazard of machine learning, particularly with time-series data, where researchers use information that would not be available during training to the test set.

In machine learning, you build a model based on data from your training set, in our case from 2015-2018, and then test your model on an unseen test set, in our case 2019 data. Our baseline model was a simple random forest classifier a small set of predictors and default parameters using 3 months of training data from 2015, which had a f-beta score of 0%, which is equivalent to the naive baseline that assumes all flights are on-time. Our final model shows marked improvement from baseline, using a more predictive set of features and downsampling the majority class, predicting the full 2015-2018 training with a f-beta of 63%. We used this final model built on the training set to predict the unseen, held-out 2019 data, which is the best and final assessment of the power of our model, achieving a middling f-beta score of 54%. In our current form, our model's f-beta metric is insufficient to provide a useful tool for industry.  This implies that our features are not rich enough to represent the complex set of factors that cause flight delays. The report's conclusion discusses several directions for future work to improve this predictive tool.

